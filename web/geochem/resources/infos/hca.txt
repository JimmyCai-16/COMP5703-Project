*What Is This Method?
Hierarchical clustering analysis is a clustering method that creates a hierarchy of clusters. This method allows one to describe the similarity of different observations in a dataset providing insight into their clustering. There are two main approaches to this analysis, the approach we use here is the bottom-up approach. The similarity of two data points is determined based on proximity or distance (Euclidean distance is used in our algorithm).
The dendrogram above is a representation of how clustered certain variables are to each other. If you were to cut the tree horizontally at any stage, you would be able to cluster the elements into groups by which sub trees you have created with your cut. Each cluster contains data points that are most similar to each other. The further to the top the tree is cut, the fewer clusters there are but also the less accuracy that clustering has on the data, and cutting it further down works in an inverse fashion.
*The Algorithm.
To begin, each datapoint is assigned its own cluster. This is a trivial clustering and does not provide any meaningful relationships beyond what is already known. Then, for each pair of clusters, they are tested to see how 'close' they are. This is calculated using a distance metric (Euclidean distance is used in our algorithm). There must also be some criteria for when specifically to link two clusters together. The most common one used is the maximum distance between two clusters. Once this passes a certain defined threshold, the two clusters will be merged. This process is repeated until one cluster is formed containing every datapoint from the inputted dataset.