*What Is This Method?
Random forests are an ensemble learning method that can be used to solve regression and classification tasks. Multiple decision trees are made in the training phase and are all used in determining the output for any data entered into the random forest model. Unlike other decision tree methods, random forests don't have a problem of over-fitting to the dataset, and can often outperform other similar methods. This is generally at the cost of accuracy, however this is heavily dependent on the context.
*The Algorithm
Random forests use multiple decision trees each with random subsets of the data in order to increase performance. A set of multiple trees is called a forest, so this is why this algorithm is called Random Forests. A decision tree is a chain of conditions, much like a flow chart, that directs the user which branch to go down based on how that condition is met, which will eventually lead the user to an answer. Decision tree learning is a general term that refers to the methods of training or teaching these decision trees from an example dataset. The advantage to having a model like this is that it is very easy to see the logic of the model, however in practice with large and complicated datasets it is often unwise to visualise the tree itself.
So how does this specifically work? First, consider the dataset as two separate variables, the target variable, which is the column that is to be predicted, and the rest of the data that is to be used as predictors. Subsets of this data are selected randomly with replacement, and then for each of those subsets, a decision tree is trained, so now a number of decision trees have been trained on different subsets of the data. These trees define the forest, and overall the model that has been created.
*How can this be used to make predictions?
If a classification task is being solved, each decision tree is fed the input and will individually output a decision, namely the class predicted. The overall prediction of the model will be the mode of these values.
If a regression task is being solved, each decision tree is fed the input, but now the average of all individual results is taken as the model's prediction.
