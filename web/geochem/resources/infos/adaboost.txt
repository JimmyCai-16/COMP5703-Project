*What is this method?
AdaBoost, short for adaptive boosting, is a method that makes predictions using a series of weak models (ones that are simple and generally inaccurate) where each model is constructed to fix the mistakes of the previous model or, in other words, to make accurate predictions about data points from which inaccurate predictions have been made by previous models. 
One of the benefits of AdaBoost is it has ways of tackling highly dimensional datasets, by only considering those features that have been shown to improve the predictive power of the model. It is also useful as an 'out of box' method, because it is uncommon that the hyperparameters of the model need to be adjusted in any major fashion. 
*The Algorithm
What this algorithm does is that it starts by building a first model (by default, in our implementation, these are decision trees) and gives equal importance to all the data points. To be specific, each model built is simple and generally makes poor predictions. It then gives greater importance to points that are wrongly classified and less importance to points that are correctly classified. If the first model is unable to correctly make a prediction given a specific data point, then the second model will place a greater importance on correctly predicting the output given that specific data point. This process continues until a maximum number of models have been constructed. 
The final prediction will be a weighted sum of the predictions from all the models constructed.
